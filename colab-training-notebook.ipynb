{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spaced Repetition ML Model Training (Google Colab)\n",
    "\n",
    "This notebook trains your spaced repetition interval prediction model using:\n",
    "- **51 advanced features** (8 base + 43 engineered)\n",
    "- **Architecture**: 51 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 1\n",
    "- **GPU acceleration** for faster training\n",
    "- **Export to TensorFlow.js** format for your Node.js server\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. Add your MongoDB connection string below\n",
    "3. Run all cells in order\n",
    "4. Download the trained model at the end"
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Install Dependencies"
   ],
   "metadata": {
    "id": "install-deps"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pymongo tensorflow tensorflowjs numpy pandas matplotlib -q\n",
    "print(\"âœ“ Dependencies installed\")"
   ],
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Configuration"
   ],
   "metadata": {
    "id": "config-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# MongoDB Configuration\n",
    "# Replace with your MongoDB Atlas connection string\n",
    "MONGODB_URI = \"mongodb+srv://username:password@cluster.mongodb.net/spaced-repetition?retryWrites=true&w=majority\"\n",
    "\n",
    "# Training Configuration\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Validation Split: {VALIDATION_SPLIT * 100}%\")"
   ],
   "metadata": {
    "id": "config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Advanced Feature Engineering\n",
    "\n",
    "Implements the same 51 features as your Node.js model"
   ],
   "metadata": {
    "id": "features-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_forgetting_curve_features(memory_strength, time_since_last_review, success_rate):\n",
    "    \"\"\"Calculate forgetting curve features based on Ebbinghaus model\"\"\"\n",
    "    decay_rate = time_since_last_review / max(memory_strength, 0.1)\n",
    "    forgetting_curve = math.exp(-decay_rate)\n",
    "    \n",
    "    learner_strength = success_rate * 2\n",
    "    adjusted_decay = math.exp(-decay_rate / max(learner_strength, 0.1))\n",
    "    \n",
    "    log_time_decay = math.log1p(decay_rate)\n",
    "    log_memory_strength = math.log1p(memory_strength)\n",
    "    \n",
    "    return {\n",
    "        'forgettingCurve': forgetting_curve,\n",
    "        'adjustedDecay': adjusted_decay,\n",
    "        'logTimeDecay': log_time_decay,\n",
    "        'logMemoryStrength': log_memory_strength,\n",
    "        'decayRate': decay_rate\n",
    "    }\n",
    "\n",
    "def calculate_interaction_features(features):\n",
    "    \"\"\"Create interaction features (products of important features)\"\"\"\n",
    "    ms = features['memoryStrength']\n",
    "    dr = features['difficultyRating']\n",
    "    ts = features['timeSinceLastReview']\n",
    "    sr = features['successRate']\n",
    "    art = features['averageResponseTime'] / 1000  # Convert to seconds\n",
    "    tr = features['totalReviews']\n",
    "    cc = features['consecutiveCorrect']\n",
    "    \n",
    "    return {\n",
    "        'difficultyTimeProduct': dr * ts,\n",
    "        'difficultyMemoryProduct': dr * ms,\n",
    "        'successMemoryProduct': sr * ms,\n",
    "        'successTimeProduct': sr * ts,\n",
    "        'responseTimeDifficultyProduct': art * dr,\n",
    "        'responseTimeMemoryProduct': art * ms,\n",
    "        'consecutiveMemoryProduct': cc * ms,\n",
    "        'consecutiveDifficultyRatio': cc / dr if dr > 0 else cc,\n",
    "        'experienceSuccessProduct': tr * sr,\n",
    "        'experienceDifficultyRatio': tr / (dr + 1) if dr > 0 else tr\n",
    "    }\n",
    "\n",
    "def calculate_polynomial_features(features):\n",
    "    \"\"\"Create polynomial features (squares and higher-order terms)\"\"\"\n",
    "    ms = features['memoryStrength']\n",
    "    dr = features['difficultyRating']\n",
    "    ts = features['timeSinceLastReview']\n",
    "    sr = features['successRate']\n",
    "    tr = features['totalReviews']\n",
    "    \n",
    "    return {\n",
    "        'memoryStrengthSquared': ms * ms,\n",
    "        'difficultySquared': dr * dr,\n",
    "        'timeSquared': ts * ts,\n",
    "        'successRateSquared': sr * sr,\n",
    "        'memoryStrengthCubed': ms ** 3,\n",
    "        'sqrtMemoryStrength': math.sqrt(ms),\n",
    "        'sqrtTotalReviews': math.sqrt(tr),\n",
    "        'inverseMemoryStrength': 1 / ms if ms > 0 else 0,\n",
    "        'inverseDifficulty': 1 / dr if dr > 0.01 else 100\n",
    "    }\n",
    "\n",
    "def encode_cyclical_time(time_of_day):\n",
    "    \"\"\"Encode time of day with sinusoidal features\"\"\"\n",
    "    radians = time_of_day * 2 * math.pi\n",
    "    \n",
    "    return {\n",
    "        'timeOfDaySin': math.sin(radians),\n",
    "        'timeOfDayCos': math.cos(radians),\n",
    "        'isMorning': 1 if 0.25 <= time_of_day < 0.5 else 0,\n",
    "        'isAfternoon': 1 if 0.5 <= time_of_day < 0.75 else 0,\n",
    "        'isEvening': 1 if time_of_day >= 0.75 or time_of_day < 0.25 else 0\n",
    "    }\n",
    "\n",
    "def calculate_moving_average_features(review_history, current_index=None):\n",
    "    \"\"\"Calculate moving average features from review history\"\"\"\n",
    "    if not review_history or len(review_history) == 0:\n",
    "        return {\n",
    "            'recentSuccessRate': 0,\n",
    "            'recentAvgResponseTime': 0,\n",
    "            'performanceTrend': 0,\n",
    "            'difficultyTrend': 0,\n",
    "            'velocityTrend': 0\n",
    "        }\n",
    "    \n",
    "    lookback_window = 5\n",
    "    if current_index is not None:\n",
    "        start_idx = max(0, current_index - lookback_window)\n",
    "        reviews = review_history[start_idx:current_index + 1]\n",
    "    else:\n",
    "        reviews = review_history[-lookback_window:]\n",
    "    \n",
    "    if len(reviews) == 0:\n",
    "        return {\n",
    "            'recentSuccessRate': 0,\n",
    "            'recentAvgResponseTime': 0,\n",
    "            'performanceTrend': 0,\n",
    "            'difficultyTrend': 0,\n",
    "            'velocityTrend': 0\n",
    "        }\n",
    "    \n",
    "    # Recent success rate\n",
    "    recent_successes = sum(1 for r in reviews if r.get('recalled', False))\n",
    "    recent_success_rate = recent_successes / len(reviews)\n",
    "    \n",
    "    # Recent average response time\n",
    "    recent_avg_response_time = sum(r.get('responseTime', 0) for r in reviews) / len(reviews)\n",
    "    \n",
    "    # Performance trend\n",
    "    performance_trend = 0\n",
    "    if len(reviews) >= 4:\n",
    "        midpoint = len(reviews) // 2\n",
    "        first_half_success = sum(1 for r in reviews[:midpoint] if r.get('recalled', False)) / midpoint\n",
    "        second_half_success = sum(1 for r in reviews[midpoint:] if r.get('recalled', False)) / (len(reviews) - midpoint)\n",
    "        performance_trend = second_half_success - first_half_success\n",
    "    \n",
    "    # Difficulty trend\n",
    "    difficulty_trend = 0\n",
    "    if len(reviews) >= 2:\n",
    "        intervals = [r.get('intervalUsed', 1) for r in reviews]\n",
    "        difficulty_trend = (intervals[-1] - intervals[0]) / max(intervals[0], 1)\n",
    "    \n",
    "    # Velocity trend\n",
    "    velocity_trend = 0\n",
    "    if len(reviews) >= 3:\n",
    "        time_diffs = []\n",
    "        for i in range(1, len(reviews)):\n",
    "            t1 = reviews[i].get('timestamp', 0)\n",
    "            t2 = reviews[i-1].get('timestamp', 0)\n",
    "            if isinstance(t1, datetime):\n",
    "                t1 = t1.timestamp() * 1000\n",
    "            if isinstance(t2, datetime):\n",
    "                t2 = t2.timestamp() * 1000\n",
    "            time_diffs.append(t1 - t2)\n",
    "        avg_time_diff = sum(time_diffs) / len(time_diffs)\n",
    "        velocity_trend = avg_time_diff / (1000 * 60 * 60 * 24)  # Convert to days\n",
    "    \n",
    "    return {\n",
    "        'recentSuccessRate': recent_success_rate,\n",
    "        'recentAvgResponseTime': recent_avg_response_time,\n",
    "        'performanceTrend': performance_trend,\n",
    "        'difficultyTrend': difficulty_trend,\n",
    "        'velocityTrend': velocity_trend\n",
    "    }\n",
    "\n",
    "def calculate_momentum_features(features, moving_avg_features):\n",
    "    \"\"\"Calculate momentum features (learning acceleration)\"\"\"\n",
    "    sr = features['successRate']\n",
    "    cc = features['consecutiveCorrect']\n",
    "    tr = features['totalReviews']\n",
    "    rsr = moving_avg_features['recentSuccessRate']\n",
    "    pt = moving_avg_features['performanceTrend']\n",
    "    \n",
    "    return {\n",
    "        'learningMomentum': rsr - sr,\n",
    "        'streakStrength': cc / math.sqrt(tr) if tr > 0 else 0,\n",
    "        'performanceAcceleration': pt,\n",
    "        'masteryLevel': sr * (1 - abs(rsr - sr))\n",
    "    }\n",
    "\n",
    "def calculate_retention_features(features, forgetting_curve_features):\n",
    "    \"\"\"Calculate retention prediction features\"\"\"\n",
    "    ms = features['memoryStrength']\n",
    "    dr = features['difficultyRating']\n",
    "    sr = features['successRate']\n",
    "    cc = features['consecutiveCorrect']\n",
    "    tr = features['totalReviews']\n",
    "    fc = forgetting_curve_features['forgettingCurve']\n",
    "    \n",
    "    stability = math.log1p(cc) * math.log1p(ms)\n",
    "    retrievability = fc * (1 - dr)\n",
    "    learning_efficiency = sr / math.log1p(tr) if tr > 0 else 0\n",
    "    retention_probability = min(1, retrievability * (1 + stability * 0.1))\n",
    "    optimal_interval_estimate = max(1, ms * abs(math.log(0.9)) * (1 + stability * 0.1))\n",
    "    \n",
    "    return {\n",
    "        'stability': stability,\n",
    "        'retrievability': retrievability,\n",
    "        'learningEfficiency': learning_efficiency,\n",
    "        'retentionProbability': retention_probability,\n",
    "        'optimalIntervalEstimate': optimal_interval_estimate\n",
    "    }\n",
    "\n",
    "def create_advanced_feature_vector(base_features, review_history=None, current_index=None):\n",
    "    \"\"\"Master function to create all 51 advanced features\"\"\"\n",
    "    # 1. Forgetting curve features (5)\n",
    "    forgetting_curve_features = calculate_forgetting_curve_features(\n",
    "        base_features['memoryStrength'],\n",
    "        base_features['timeSinceLastReview'],\n",
    "        base_features['successRate']\n",
    "    )\n",
    "    \n",
    "    # 2. Interaction features (10)\n",
    "    interaction_features = calculate_interaction_features(base_features)\n",
    "    \n",
    "    # 3. Polynomial features (9)\n",
    "    polynomial_features = calculate_polynomial_features(base_features)\n",
    "    \n",
    "    # 4. Cyclical time encoding (5)\n",
    "    time_features = encode_cyclical_time(base_features['timeOfDay'])\n",
    "    \n",
    "    # 5. Moving average features (5)\n",
    "    moving_avg_features = calculate_moving_average_features(review_history, current_index)\n",
    "    \n",
    "    # 6. Momentum features (4)\n",
    "    momentum_features = calculate_momentum_features(base_features, moving_avg_features)\n",
    "    \n",
    "    # 7. Retention features (5)\n",
    "    retention_features = calculate_retention_features(base_features, forgetting_curve_features)\n",
    "    \n",
    "    # Combine all features (8 base + 43 advanced = 51 total)\n",
    "    all_features = {\n",
    "        # Base features (8)\n",
    "        'memoryStrength': base_features['memoryStrength'],\n",
    "        'difficultyRating': base_features['difficultyRating'],\n",
    "        'timeSinceLastReview': base_features['timeSinceLastReview'],\n",
    "        'successRate': base_features['successRate'],\n",
    "        'averageResponseTime': base_features['averageResponseTime'] / 1000,  # Convert to seconds\n",
    "        'totalReviews': base_features['totalReviews'],\n",
    "        'consecutiveCorrect': base_features['consecutiveCorrect'],\n",
    "        'timeOfDay': base_features['timeOfDay']\n",
    "    }\n",
    "    \n",
    "    # Add all engineered features\n",
    "    all_features.update(forgetting_curve_features)\n",
    "    all_features.update(interaction_features)\n",
    "    all_features.update(polynomial_features)\n",
    "    all_features.update(time_features)\n",
    "    all_features.update(moving_avg_features)\n",
    "    all_features.update(momentum_features)\n",
    "    all_features.update(retention_features)\n",
    "    \n",
    "    # Return as ordered array (51 features)\n",
    "    feature_order = [\n",
    "        # Base (8)\n",
    "        'memoryStrength', 'difficultyRating', 'timeSinceLastReview', 'successRate',\n",
    "        'averageResponseTime', 'totalReviews', 'consecutiveCorrect', 'timeOfDay',\n",
    "        # Forgetting curve (5)\n",
    "        'forgettingCurve', 'adjustedDecay', 'logTimeDecay', 'logMemoryStrength', 'decayRate',\n",
    "        # Interaction (10)\n",
    "        'difficultyTimeProduct', 'difficultyMemoryProduct', 'successMemoryProduct', 'successTimeProduct',\n",
    "        'responseTimeDifficultyProduct', 'responseTimeMemoryProduct', 'consecutiveMemoryProduct',\n",
    "        'consecutiveDifficultyRatio', 'experienceSuccessProduct', 'experienceDifficultyRatio',\n",
    "        # Polynomial (9)\n",
    "        'memoryStrengthSquared', 'difficultySquared', 'timeSquared', 'successRateSquared',\n",
    "        'memoryStrengthCubed', 'sqrtMemoryStrength', 'sqrtTotalReviews',\n",
    "        'inverseMemoryStrength', 'inverseDifficulty',\n",
    "        # Cyclical time (5)\n",
    "        'timeOfDaySin', 'timeOfDayCos', 'isMorning', 'isAfternoon', 'isEvening',\n",
    "        # Moving average (5)\n",
    "        'recentSuccessRate', 'recentAvgResponseTime', 'performanceTrend',\n",
    "        'difficultyTrend', 'velocityTrend',\n",
    "        # Momentum (4)\n",
    "        'learningMomentum', 'streakStrength', 'performanceAcceleration', 'masteryLevel',\n",
    "        # Retention (5)\n",
    "        'stability', 'retrievability', 'learningEfficiency', 'retentionProbability',\n",
    "        'optimalIntervalEstimate'\n",
    "    ]\n",
    "    \n",
    "    # Convert response time for moving avg features\n",
    "    if 'recentAvgResponseTime' in all_features:\n",
    "        all_features['recentAvgResponseTime'] = all_features['recentAvgResponseTime'] / 1000\n",
    "    \n",
    "    return [all_features[key] for key in feature_order]\n",
    "\n",
    "print(\"âœ“ Feature engineering functions loaded (51 features)\")"
   ],
   "metadata": {
    "id": "features"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Extract Training Data from MongoDB"
   ],
   "metadata": {
    "id": "extract-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_training_data_from_mongodb(mongodb_uri):\n",
    "    \"\"\"Extract training data from MongoDB (matches Node.js extraction logic)\"\"\"\n",
    "    print(\"Connecting to MongoDB...\")\n",
    "    client = MongoClient(mongodb_uri)\n",
    "    db = client.get_default_database()\n",
    "    users_collection = db['users']\n",
    "    \n",
    "    print(\"Extracting training data...\\n\")\n",
    "    \n",
    "    users = list(users_collection.find({}))\n",
    "    training_data = []\n",
    "    total_reviews = 0\n",
    "    \n",
    "    for user in users:\n",
    "        questions = user.get('questions', [])\n",
    "        \n",
    "        for question in questions:\n",
    "            review_history = question.get('reviewHistory', [])\n",
    "            \n",
    "            if len(review_history) < 2:\n",
    "                continue  # Need at least 2 reviews\n",
    "            \n",
    "            # For each review (except the last), create a training sample\n",
    "            for i in range(len(review_history) - 1):\n",
    "                current_review = review_history[i]\n",
    "                next_review = review_history[i + 1]\n",
    "                \n",
    "                # Calculate days between reviews\n",
    "                t1 = current_review['timestamp']\n",
    "                t2 = next_review['timestamp']\n",
    "                if isinstance(t1, datetime):\n",
    "                    t1 = t1.timestamp() * 1000\n",
    "                if isinstance(t2, datetime):\n",
    "                    t2 = t2.timestamp() * 1000\n",
    "                \n",
    "                time_diff = t2 - t1\n",
    "                days_between = time_diff / (1000 * 60 * 60 * 24)\n",
    "                \n",
    "                # Calculate stats at time of review\n",
    "                reviews_up_to_now = review_history[:i + 1]\n",
    "                correct_up_to_now = sum(1 for r in reviews_up_to_now if r.get('recalled', False))\n",
    "                success_rate = correct_up_to_now / len(reviews_up_to_now)\n",
    "                \n",
    "                # Calculate time since previous review\n",
    "                if i > 0:\n",
    "                    prev_t = reviews_up_to_now[i - 1]['timestamp']\n",
    "                    curr_t = current_review['timestamp']\n",
    "                    if isinstance(prev_t, datetime):\n",
    "                        prev_t = prev_t.timestamp() * 1000\n",
    "                    if isinstance(curr_t, datetime):\n",
    "                        curr_t = curr_t.timestamp() * 1000\n",
    "                    time_since_last_review = (curr_t - prev_t) / (1000 * 60 * 60 * 24)\n",
    "                else:\n",
    "                    time_since_last_review = 0\n",
    "                \n",
    "                # Calculate consecutive correct\n",
    "                consecutive_correct = 0\n",
    "                if current_review.get('recalled', False):\n",
    "                    for r in reversed(reviews_up_to_now[:i]):\n",
    "                        if r.get('recalled', False):\n",
    "                            consecutive_correct += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    consecutive_correct += 1  # Include current review\n",
    "                \n",
    "                # Calculate time of day\n",
    "                timestamp = current_review['timestamp']\n",
    "                if isinstance(timestamp, datetime):\n",
    "                    hour = timestamp.hour\n",
    "                else:\n",
    "                    hour = datetime.fromtimestamp(timestamp / 1000).hour\n",
    "                time_of_day = hour / 24\n",
    "                \n",
    "                # Feature vector at time of current review\n",
    "                base_features = {\n",
    "                    'memoryStrength': current_review.get('intervalUsed', 1),\n",
    "                    'difficultyRating': 1 - success_rate,\n",
    "                    'timeSinceLastReview': time_since_last_review,\n",
    "                    'successRate': success_rate,\n",
    "                    'averageResponseTime': sum(r.get('responseTime', 0) for r in reviews_up_to_now) / len(reviews_up_to_now),\n",
    "                    'totalReviews': len(reviews_up_to_now),\n",
    "                    'consecutiveCorrect': consecutive_correct,\n",
    "                    'timeOfDay': time_of_day\n",
    "                }\n",
    "                \n",
    "                # Generate advanced features (51 dimensions)\n",
    "                feature_vector = create_advanced_feature_vector(base_features, review_history, i)\n",
    "                \n",
    "                # Label: optimal interval\n",
    "                recalled = next_review.get('recalled', False)\n",
    "                optimal_interval = math.ceil(days_between * 1.2) if recalled else max(1, math.floor(days_between * 0.7))\n",
    "                \n",
    "                training_data.append({\n",
    "                    'features': feature_vector,\n",
    "                    'label': optimal_interval,\n",
    "                    'metadata': {\n",
    "                        'question': question.get('question', ''),\n",
    "                        'recalled': recalled,\n",
    "                        'actualInterval': days_between\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                total_reviews += 1\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    print(f\"âœ“ Extracted {len(training_data)} training samples\")\n",
    "    print(f\"  From {total_reviews} total reviews\")\n",
    "    print(f\"  Across {len(users)} user(s)\\n\")\n",
    "    \n",
    "    if len(training_data) == 0:\n",
    "        print(\"âš ï¸  No training data found!\")\n",
    "        print(\"   Make sure users have review history in MongoDB.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Statistics\n",
    "    recalled = sum(1 for d in training_data if d['metadata']['recalled'])\n",
    "    retention_rate = recalled / len(training_data)\n",
    "    avg_interval = sum(d['metadata']['actualInterval'] for d in training_data) / len(training_data)\n",
    "    \n",
    "    print(f\"Training Data Statistics:\")\n",
    "    print(f\"  Retention Rate: {retention_rate * 100:.1f}%\")\n",
    "    print(f\"  Average Interval: {avg_interval:.2f} days\\n\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array([d['features'] for d in training_data], dtype=np.float32)\n",
    "    y = np.array([d['label'] for d in training_data], dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"âœ“ Data extraction function loaded\")"
   ],
   "metadata": {
    "id": "extract"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract training data from your MongoDB\n",
    "X, y = extract_training_data_from_mongodb(MONGODB_URI)\n",
    "\n",
    "if X is not None:\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Label vector shape: {y.shape}\")\n",
    "    print(f\"\\nFeature range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "    print(f\"Label range: [{y.min():.2f}, {y.max():.2f}]\")"
   ],
   "metadata": {
    "id": "run-extract"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Build Model Architecture\n",
    "\n",
    "**Architecture**: 51 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 1\n",
    "\n",
    "Matches your Node.js model exactly"
   ],
   "metadata": {
    "id": "model-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create the neural network (matches Node.js model architecture)\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer: 51 features\n",
    "        layers.Dense(128, activation='relu', kernel_initializer='he_normal', input_shape=(51,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        layers.Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
    "        \n",
    "        # Output layer: predict interval (days)\n",
    "        # Using softplus to ensure positive output\n",
    "        layers.Dense(1, activation='softplus')\n",
    "    ])\n",
    "    \n",
    "    # Compile with Adam optimizer and MSE loss\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Architecture\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "print(\"=\"*60)"
   ],
   "metadata": {
    "id": "model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Prepare Data for Training"
   ],
   "metadata": {
    "id": "prepare-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if X is not None:\n",
    "    # Split data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize features (z-score normalization)\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0) + 1e-8\n",
    "    \n",
    "    X_train_normalized = (X_train - mean) / std\n",
    "    X_test_normalized = (X_test - mean) / std\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"\\nNormalization stats:\")\n",
    "    print(f\"  Mean range: [{mean.min():.2f}, {mean.max():.2f}]\")\n",
    "    print(f\"  Std range: [{std.min():.2f}, {std.max():.2f}]\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot prepare data - no training samples available\")"
   ],
   "metadata": {
    "id": "prepare"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Train the Model"
   ],
   "metadata": {
    "id": "train-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if X is not None:\n",
    "    import time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Model\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"Validation Split: {VALIDATION_SPLIT * 100}%\")\n",
    "    print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Learning rate reduction on plateau\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_normalized,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ“ Training complete in {training_time:.2f}s\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot train - no training data available\")"
   ],
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Evaluate Model Performance"
   ],
   "metadata": {
    "id": "eval-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if X is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_mae = model.evaluate(X_test_normalized, y_test, verbose=0)\n",
    "    \n",
    "    # Calculate baseline performance (just using current memory strength)\n",
    "    baseline_predictions = X_test[:, 0]  # First feature is memoryStrength\n",
    "    baseline_mae = np.mean(np.abs(baseline_predictions - y_test))\n",
    "    \n",
    "    improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model Performance\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f} days\")\n",
    "    print(f\"\\nBaseline MAE: {baseline_mae:.4f} days\")\n",
    "    print(f\"Improvement: {improvement:.1f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.set_title('Model Loss Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE plot\n",
    "    ax2.plot(history.history['mae'], label='Training MAE')\n",
    "    ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAE (days)')\n",
    "    ax2.set_title('Model MAE Over Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Training history saved as 'training_history.png'\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot evaluate - no test data available\")"
   ],
   "metadata": {
    "id": "eval"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Sample Predictions"
   ],
   "metadata": {
    "id": "predict-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if X is not None:\n",
    "    # Make predictions on test set\n",
    "    predictions = model.predict(X_test_normalized, verbose=0).flatten()\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Predicted':>12} {'Actual':>12} {'Error':>12} {'Memory Strength':>18}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    indices = [0, len(predictions)//4, len(predictions)//2, 3*len(predictions)//4, len(predictions)-1]\n",
    "    \n",
    "    for i in indices:\n",
    "        if i < len(predictions):\n",
    "            pred = predictions[i]\n",
    "            actual = y_test[i]\n",
    "            error = abs(pred - actual)\n",
    "            memory_strength = X_test[i, 0]  # First feature\n",
    "            print(f\"{pred:12.2f} {actual:12.2f} {error:12.2f} {memory_strength:18.2f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Scatter plot: predicted vs actual\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_test, predictions, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Interval (days)', fontsize=12)\n",
    "    plt.ylabel('Predicted Interval (days)', fontsize=12)\n",
    "    plt.title('Predicted vs Actual Intervals', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Predictions plot saved as 'predictions.png'\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot predict - no test data available\")"
   ],
   "metadata": {
    "id": "predict"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Export Model to TensorFlow.js Format"
   ],
   "metadata": {
    "id": "export-header"
   }
  },
  {
   "cell_type": "code",
   "source": "if X is not None:\n    import tensorflowjs as tfjs\n    import json\n    \n    # Export model to TensorFlow.js format\n    print(\"\\nExporting model to TensorFlow.js format...\")\n    tfjs.converters.save_keras_model(model, './tfjs_model')\n    print(\"âœ“ Model exported to './tfjs_model'\")\n    \n    # Save normalization stats (needed for inference)\n    normalization_stats = {\n        'mean': mean.tolist(),\n        'std': std.tolist()\n    }\n    \n    with open('./tfjs_model/normalization-stats.json', 'w') as f:\n        json.dump(normalization_stats, f, indent=2)\n    \n    print(\"âœ“ Normalization stats saved\")\n    \n    # Create metadata file\n    metadata = {\n        'modelVersion': '1.0.0',\n        'trainedDate': datetime.now().isoformat(),\n        'numFeatures': 51,\n        'architecture': '51â†’128â†’64â†’32â†’16â†’1',\n        'trainingSize': len(X_train),\n        'testSize': len(X_test),\n        'performance': {\n            'testMAE': float(test_mae),\n            'testLoss': float(test_loss),\n            'baselineMAE': float(baseline_mae),\n            'improvement': float(improvement)\n        },\n        'training': {\n            'epochs': EPOCHS,\n            'batchSize': BATCH_SIZE,\n            'learningRate': LEARNING_RATE,\n            'validationSplit': VALIDATION_SPLIT,\n            'trainingTime': training_time\n        }\n    }\n    \n    with open('./tfjs_model/metadata.json', 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(\"âœ“ Metadata saved\")\n    \n    # Zip the model for download\n    !zip -r spaced_repetition_model.zip tfjs_model/\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Model Export Complete!\")\n    print(\"=\"*60)\n    print(\"\\nFiles created:\")\n    print(\"  ðŸ“¦ spaced_repetition_model.zip\")\n    print(\"  ðŸ“ tfjs_model/\")\n    print(\"     - model.json\")\n    print(\"     - group1-shard*.bin\")\n    print(\"     - normalization-stats.json\")\n    print(\"     - metadata.json\")\n    print(\"\\nNext Steps:\")\n    print(\"  1. Download 'spaced_repetition_model.zip'\")\n    print(\"  2. Extract to your server: ml/saved-model/\")\n    print(\"  3. Restart your Node.js server\")\n    print(\"  4. Model will be loaded automatically!\")\n    print(\"=\"*60)\nelse:\n    print(\"âš ï¸  Cannot export - no trained model available\")",
   "metadata": {
    "id": "export"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Download Model\n",
    "\n",
    "Run this cell to download the trained model to your computer"
   ],
   "metadata": {
    "id": "download-header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if X is not None:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Preparing model for download...\")\n",
    "    files.download('spaced_repetition_model.zip')\n",
    "    print(\"âœ“ Download started!\")\n",
    "else:\n",
    "    print(\"âš ï¸  No model to download\")"
   ],
   "metadata": {
    "id": "download"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- âœ… Connects to your MongoDB Atlas database\n",
    "- âœ… Extracts training data from review history\n",
    "- âœ… Engineers 51 features (matches your Node.js model)\n",
    "- âœ… Trains model with GPU acceleration\n",
    "- âœ… Exports to TensorFlow.js format\n",
    "- âœ… Ready for deployment on your server\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "1. **More data = better model**: Aim for 100+ training samples\n",
    "2. **Experiment with architecture**: Try different layer sizes\n",
    "3. **Tune hyperparameters**: Adjust learning rate, batch size, epochs\n",
    "4. **Monitor overfitting**: Check validation loss vs training loss\n",
    "5. **Retrain periodically**: As users generate more reviews\n",
    "\n",
    "### Next Experiments to Try\n",
    "\n",
    "- Add LSTM layers for temporal patterns\n",
    "- Try different activation functions\n",
    "- Implement attention mechanisms\n",
    "- Use data augmentation\n",
    "- Test ensemble models\n"
   ],
   "metadata": {
    "id": "summary"
   }
  }
 ]
}